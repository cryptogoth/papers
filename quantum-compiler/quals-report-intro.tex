\section{Introduction}

Quantum computers can achieve amazing exponential speedups over known classical
algorithms in theory. For example, Shor's factoring algorithm can break the
widely-used RSA cryptosystem, and quantum many-body systems can only be
efficiently simulated by quantum algorithms [paper citations needed].
However, this speedup can be mitigated or lost completely when translating
these algorithms into a physical experiment.
Two important considerations in this implementation process are fault-tolerance
and architecture; both of these introduce their own resource overheads and
complications, but both are ultimately necessary to build a working quantum
computer.
Fault-tolerance (including error correction) is a vast and fascinating topic
that we unfortunately do not address here but take for granted.
Architecture (including compilation) is
concerned with the physical layout and interaction of qubits (or qudits) in
order to execute algorithms efficiently. In actuality, fault-tolerance and
architecture will probably be inter-related concerns, but for now we concern 
ourselves with the more modest task of efficient compilation.

\emph{Quantum compiling} is the
approximation of a high-level quantum algorithm (usually described as a
reversible circuit) to a sequence
of low-level, universal quantum gates that depend on our hardware, the
"assembly language" of quantum computing.
At the highest-level of abstraction, all quantum algorithms on $n$ qubits
can be considered as a unitary matrix of dimension $2^n \times 2^n$ with
unit determinant, followed by measurement (and perhaps several rounds of this).
However, in experimental settings,
we can only perform some gates efficiently, and these are usually local
two-qubit or single-qubit operations.
Moreover, most of our results for fault-tolerant
quantum computing in the presence of noise stipulates that we have a finite
number of universal gates we can perform with some limited precision.

Several quantum compiling procedures are currently known. The first one,
by Solovay and Kitaev (SK),
is a central results of quantum computing which states that we
can approximate quantum gates efficiently without losing any performance
gains over classical computers  \cite{Dawson2005}. That is, we only experience polynomial
overhead in the input size (polylogarithmic overhead in the inverse error)
rather than exponential overhead.
A second procedure due to Kitaev, Shen, and Vyalyi (KSV) is more recent but
less well-known, improves
both the depth and size of the Solovay-Kitaev procedure by trading
more ancillae (space) for time (depth) using parallelism
\cite{ksv02}. There are several other quantum compiling approaches known,
most notably the work of Austin Fowler \cite{Fowler2004} which combines
compiling with fault-tolerance,
which will be mentioned in the next section.
However, the KSV procedure is unique in its combination of various theoretical
tools and warrants this dedicated comparison.
Therefore, we do not consider
these other compiling procedures.

This work contributes the calculation of physical resources needed
to run the SK and KSV quantum compilers on a single-qubit phase gate
of the form $\ket{0}\bra{0} + e^{i\phi}\ket{1}\bra{1}$, for reasons that
will soon become clear. We also contribute open source code to duplicate
these results, which is available at \texttt{http://quantum-compiler.org}.
Finally, we give a pedagogical review of KSV and its building blocks from the
perspective of implementation on an architecture allowing arbitrary concurrent
operations. This model is known as \textsc{AC} in the literature.
\cite{VanMeter2008}. Therefore, this current work can be read in combination
with Chapter 13 of \cite{ksv02} to gain a full understanding of the KSV
compiler.

The rest of this report is organized as follows.
First things first, Section \ref{sec:prelims} defines terms and parameters
so that we can discuss quantum compilers with some rigor as well as
giving asymptotic bounds for specific algorithms.
Then Section
\ref{sec:related} gives a brief history of quantum compiling.
The next two sections describe the two compiling algorithms and how
to measure their relative performance.
Section \ref{sec:sk-algo} reviews the original SK result and
Section \ref{sec:main-algo} describes the building blocks of KSV in detail
along with its
most resource-intensive modules. Section \ref{sec:methods} describe
our methods for the performance comparisons, which are given in Section
\ref{sec:results}. Finally, we make some comments about these results
and suggest future directions for extending this work. Ready? Let's go.